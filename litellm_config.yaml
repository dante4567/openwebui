# LiteLLM Configuration for OpenWebUI
# Unified gateway for all LLM providers with caching, cost tracking, and fallbacks

model_list:
  # OpenAI Models
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: ${OPENAI_API_KEY}

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: ${OPENAI_API_KEY}

  # Anthropic Claude
  - model_name: claude-3-5-sonnet-20241022
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
      api_key: ${ANTHROPIC_API_KEY}

  - model_name: claude-3-5-haiku-20241022
    litellm_params:
      model: anthropic/claude-3-5-haiku-20241022
      api_key: ${ANTHROPIC_API_KEY}

  # Groq (Fast, free tier)
  - model_name: llama-3.1-70b-versatile
    litellm_params:
      model: groq/llama-3.1-70b-versatile
      api_key: ${GROQ_API_KEY}

  - model_name: llama-3.1-8b-instant
    litellm_params:
      model: groq/llama-3.1-8b-instant
      api_key: ${GROQ_API_KEY}

  # Google Gemini
  - model_name: gemini-1.5-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: ${GOOGLE_API_KEY}

  - model_name: gemini-1.5-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: ${GOOGLE_API_KEY}

# Redis caching configuration (saves API costs)
litellm_settings:
  cache: true
  cache_params:
    type: redis
    host: redis
    port: 6379

  # Cost tracking
  success_callback: ["_PROXY_track_cost_callback"]

  # Fallback logic (if primary fails, try backup)
  fallbacks: [
    {"gpt-4o": ["gpt-4o-mini"]},
    {"claude-3-5-sonnet-20241022": ["gpt-4o"]},
    {"llama-3.1-70b-versatile": ["llama-3.1-8b-instant"]}
  ]

  # Rate limiting (optional - prevents runaway costs)
  # rpm_limit: 60  # requests per minute
  # tpm_limit: 100000  # tokens per minute

  # Request timeout
  request_timeout: 600

# General settings
general_settings:
  master_key: sk-1234  # Must match LITELLM_MASTER_KEY in docker-compose
  database_url: null  # Optional: Use postgres for multi-instance deployments

  # UI settings
  ui_username: admin
  ui_password: ${LITELLM_UI_PASSWORD:-admin}  # Change in production!

  # Logging
  set_verbose: false
  json_logs: true
