# LiteLLM Configuration for OpenWebUI
# Unified gateway for all LLM providers with caching, cost tracking, and fallbacks
# Updated: 2025-10-12 - All models verified current as of October 2025

model_list:
  # ========================================
  # OpenAI Models (Updated Oct 2025)
  # ========================================
  - model_name: gpt-4.1-mini
    litellm_params:
      model: openai/gpt-4.1-mini
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true

  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true

  # ========================================
  # Anthropic Claude (Updated Oct 2025)
  # ========================================
  - model_name: claude-sonnet-4-5-20250929
    litellm_params:
      model: anthropic/claude-sonnet-4-5-20250929
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true

  - model_name: claude-3-5-sonnet-20241022
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true

  - model_name: claude-3-5-haiku-20241022
    litellm_params:
      model: anthropic/claude-3-5-haiku-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false

  # ========================================
  # Groq (Updated Oct 2025 - Fast & cheap)
  # ========================================
  - model_name: llama-3.3-70b-versatile
    litellm_params:
      model: groq/llama-3.3-70b-versatile
      api_key: os.environ/GROQ_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false

  - model_name: llama-3.1-8b-instant
    litellm_params:
      model: groq/llama-3.1-8b-instant
      api_key: os.environ/GROQ_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false

  # OpenAI GPT OSS models (hosted by Groq) - VERY FAST
  - model_name: gpt-oss-20b
    litellm_params:
      model: groq/openai/gpt-oss-20b
      api_key: os.environ/GROQ_API_KEY
      max_tokens: 131072  # Full 131k context support
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false
      # Fastest Groq model: 1000 tokens/sec
      # Pricing: $0.10 input, $0.50 output per 1M tokens
      input_cost_per_token: 0.0000001  # $0.10 per 1M
      output_cost_per_token: 0.0000005  # $0.50 per 1M
      max_input_tokens: 131072

  - model_name: gpt-oss-120b
    litellm_params:
      model: groq/openai/gpt-oss-120b
      api_key: os.environ/GROQ_API_KEY
      max_tokens: 131072  # Full 131k context support
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false
      # High-quality Groq model: 500 tokens/sec
      # Pricing: $0.15 input, $0.75 output per 1M tokens
      input_cost_per_token: 0.00000015  # $0.15 per 1M
      output_cost_per_token: 0.00000075  # $0.75 per 1M
      max_input_tokens: 131072

  # Kimi K2 - LARGEST context window on Groq (262k!)
  - model_name: kimi-k2-instruct-0905
    litellm_params:
      model: groq/moonshotai/kimi-k2-instruct-0905
      api_key: os.environ/GROQ_API_KEY
      max_tokens: 262144  # HUGE: 262k context (double the usual)
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false
      # Made by Moonshot AI (Chinese company)
      # Best for: Long document analysis, code review, summarization
      max_input_tokens: 262144

  # Groq Compound - Built-in web search + code execution
  - model_name: groq-compound
    litellm_params:
      model: groq/groq/compound
      api_key: os.environ/GROQ_API_KEY
      max_tokens: 131072
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false
      # Speed: ~450 tokens/sec
      # Features: Web search, code execution (like Perplexity)
      # Use for: Research, fact-checking, coding tasks
      max_input_tokens: 131072

  # Qwen 3 32B - Excellent multilingual support (Chinese-first)
  - model_name: qwen3-32b
    litellm_params:
      model: groq/qwen/qwen3-32b
      api_key: os.environ/GROQ_API_KEY
      max_tokens: 131072
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false
      # Made by Alibaba Cloud
      # Best for: Chinese language tasks, multilingual support
      max_input_tokens: 131072

  # ========================================
  # Google Gemini (Updated Oct 2025)
  # ========================================
  # NOTE: Gemini 2.5 models use reasoning tokens (think before responding)
  # Requires higher max_tokens: ~50-200+ tokens vs 5-10 for other models
  # Example: "Reply with OK" uses ~157 reasoning + 1 text = 158 total tokens

  - model_name: gemini-2.5-pro
    litellm_params:
      model: gemini/gemini-2.5-pro
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true
      # Reasoning model - needs 100-300 tokens for good responses

  - model_name: gemini-2.5-flash
    litellm_params:
      model: gemini/gemini-2.5-flash
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true
      # Reasoning model - needs 50-200 tokens (faster than 2.5-pro)

  - model_name: gemini-2.0-flash
    litellm_params:
      model: gemini/gemini-2.0-flash
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true
      # Standard model - works with normal token limits (5-20 tokens)

# Redis caching configuration (saves API costs)
litellm_settings:
  cache: true
  cache_params:
    type: redis
    host: redis
    port: 6379

  # Cost tracking
  success_callback: ["_PROXY_track_cost_callback"]

  # Fallback logic (if primary fails, try backup)
  fallbacks: [
    {"gpt-4o": ["gpt-4.1-mini", "gpt-4o-mini"]},
    {"gpt-4.1-mini": ["gpt-4o-mini"]},
    {"claude-sonnet-4-5-20250929": ["claude-3-5-sonnet-20241022", "gpt-4o"]},
    {"claude-3-5-sonnet-20241022": ["gpt-4o"]},
    {"llama-3.3-70b-versatile": ["llama-3.1-8b-instant"]},
    {"gemini-2.5-pro": ["gemini-2.5-flash", "gpt-4o-mini"]},
    {"gemini-2.5-flash": ["gemini-2.0-flash", "gpt-4o-mini"]}
  ]

  # Rate limiting (optional - prevents runaway costs)
  # rpm_limit: 60  # requests per minute
  # tpm_limit: 100000  # tokens per minute

  # Request timeout
  request_timeout: 600

# General settings
general_settings:
  master_key: sk-1234  # Must match LITELLM_MASTER_KEY in docker-compose

  # Database disabled - LiteLLM UI requires PostgreSQL (not SQLite)
  # Use logs and API endpoints for cost tracking instead
  # database_url: null

  # Logging with cost tracking
  set_verbose: true  # Shows costs in logs
  json_logs: true
