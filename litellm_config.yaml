# LiteLLM Configuration for OpenWebUI
# Unified gateway for all LLM providers with caching, cost tracking, and fallbacks
# Updated: 2025-10-12 - All models verified current as of October 2025

model_list:
  # ========================================
  # OpenAI Models (Updated Oct 2025)
  # ========================================
  - model_name: gpt-4.1-mini
    litellm_params:
      model: openai/gpt-4.1-mini
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true

  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true

  # ========================================
  # Anthropic Claude (Updated Oct 2025)
  # ========================================
  - model_name: claude-sonnet-4-5-20250929
    litellm_params:
      model: anthropic/claude-sonnet-4-5-20250929
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true

  - model_name: claude-3-5-sonnet-20241022
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true

  - model_name: claude-3-5-haiku-20241022
    litellm_params:
      model: anthropic/claude-3-5-haiku-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false

  # ========================================
  # Groq (Updated Oct 2025 - Fast & cheap)
  # ========================================
  - model_name: llama-3.3-70b-versatile
    litellm_params:
      model: groq/llama-3.3-70b-versatile
      api_key: os.environ/GROQ_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false

  - model_name: llama-3.1-8b-instant
    litellm_params:
      model: groq/llama-3.1-8b-instant
      api_key: os.environ/GROQ_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false

  # ========================================
  # Google Gemini (Updated Oct 2025)
  # ========================================
  # NOTE: Gemini 2.5 models use reasoning tokens (think before responding)
  # Requires higher max_tokens: ~50-200+ tokens vs 5-10 for other models
  # Example: "Reply with OK" uses ~157 reasoning + 1 text = 158 total tokens

  - model_name: gemini-2.5-pro
    litellm_params:
      model: gemini/gemini-2.5-pro
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true
      # Reasoning model - needs 100-300 tokens for good responses

  - model_name: gemini-2.5-flash
    litellm_params:
      model: gemini/gemini-2.5-flash
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true
      # Reasoning model - needs 50-200 tokens (faster than 2.5-pro)

  - model_name: gemini-2.0-flash
    litellm_params:
      model: gemini/gemini-2.0-flash
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true
      # Standard model - works with normal token limits (5-20 tokens)

# Redis caching configuration (saves API costs)
litellm_settings:
  cache: true
  cache_params:
    type: redis
    host: redis
    port: 6379

  # Cost tracking
  success_callback: ["_PROXY_track_cost_callback"]

  # Fallback logic (if primary fails, try backup)
  fallbacks: [
    {"gpt-4o": ["gpt-4.1-mini", "gpt-4o-mini"]},
    {"gpt-4.1-mini": ["gpt-4o-mini"]},
    {"claude-sonnet-4-5-20250929": ["claude-3-5-sonnet-20241022", "gpt-4o"]},
    {"claude-3-5-sonnet-20241022": ["gpt-4o"]},
    {"llama-3.3-70b-versatile": ["llama-3.1-8b-instant"]},
    {"gemini-2.5-pro": ["gemini-2.5-flash", "gpt-4o-mini"]},
    {"gemini-2.5-flash": ["gemini-2.0-flash", "gpt-4o-mini"]}
  ]

  # Rate limiting (optional - prevents runaway costs)
  # rpm_limit: 60  # requests per minute
  # tpm_limit: 100000  # tokens per minute

  # Request timeout
  request_timeout: 600

# General settings
general_settings:
  master_key: sk-1234  # Must match LITELLM_MASTER_KEY in docker-compose

  # Enable database for persistent cost tracking
  database_url: "sqlite:////app/data/litellm.db"

  # Enable admin UI for cost tracking dashboard
  ui_username: admin
  ui_password: changeme123  # Change this after first login!

  # Logging
  set_verbose: false
  json_logs: true
